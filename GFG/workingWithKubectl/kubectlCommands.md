- Create a deployment named my-nginx, and each deplyment has one pod, so the pod is created automatically, and the container inside the pod is created using docker nginx image. And we have made 5 replicas of pod, in this deployment -: ```kubectl create deployment my-nginx --image=nginx --replicas=5```
- To create a pod also we will use the same command, just replicas=1 will be there, so use same command for creating pods. We don't create pods, we create deployent, which itself manages lifecycle of pod
  
---

- A pod can have multiple containers
- A deployment is has may have multiple pods, but they all are replicas. They are identical to each other. You can not create two different pods in same deployment. A deployment has exactly one Pod template
- To create two different pods, create two different deployments
- So basically deployment is just an abstraction of pods to create its replicas, and we create deployment only, we don't create pods directly
- only pods can be scaled by creating multiple replicas. So whenever we are talking about scaling and replicating, we are talking about pods, not about namespace, nodes, cluster, deployment. And when we scale pods, it is not compulsory they will go to same nodes, pods are in deployment, and deployent is not related to node, so any node can make a replica of that pod
  
---

There are 2 ways to deplot resources in kubernetes-:
1. ```kubectl create```, used in command line to directly create any resource
2. ```kubectl apply`` , used in command line to create a resource defined in manifest file
   
---

- Neither deployment nor namespace belong to one node. deployment belong to a namespace and namespace belong to a cluster, so you run this ```kubectl get namespace``` in a  or ```kubectl get deployment```. you same resuts in each node. There are different deployment in different namespaces, so for a namespace ns1, each node will give same deployments
  

---

# If i have mutiple microservices,  and i deploy them directly on aws ec2 instances without any containers, and then i applied autoscaling , and then apply load balancing. Now the question is , if i containerize the applications , then kubernetes itself has scaling and load balancing, so where to apply those scaling and load balancing? should we apply them on ec2 instances or on kubernetes pods?


## 1ï¸âƒ£ Without containers (pure EC2 model)


```
User â†’ Load Balancer â†’ EC2 instances (microservices running directly)
```

* **Scaling**:
  ðŸ‘‰ Auto Scaling Group (ASG) adds/removes **EC2 instances**
* **Load balancing**:
  ðŸ‘‰ AWS ALB / NLB distributes traffic across EC2s
* **Unit of scaling**: EC2 instance

---

## 2ï¸âƒ£ With containers + Kubernetes (EKS or self-managed)

Now Kubernetes introduces **two layers of scaling and load balancing**.

```
User
 â†“
AWS Load Balancer (ALB / NLB)
 â†“
Kubernetes Service
 â†“
Pods (containers)
 â†“
EC2 worker nodes
```

---

## 3ï¸âƒ£ Where should scaling happen?

### âœ… **Answer: BOTH â€” but at different layers**

---

## ðŸ”¹ Layer 1: Pod-level scaling (PRIMARY scaling)

This is **Kubernetesâ€™ main responsibility**.

### ðŸ‘‰ Use: **HPA (Horizontal Pod Autoscaler)**

* Scales **Pods**
* Based on:

  * CPU / memory
  * Custom metrics (requests/sec, queue length, etc.)
* Fast and fine-grained

Example:

```
Service A:
  minPods: 2
  maxPods: 20
```

âœ” Best for handling traffic spikes
âœ” No EC2 creation needed
âœ” Very fast

ðŸŸ¢ **This is where your microservices should scale first**

---

## ðŸ”¹ Layer 2: Node-level scaling (SECONDARY scaling)

Pods need **EC2 capacity** to run.

### ðŸ‘‰ Use: **Cluster Autoscaler (or Karpenter)**

* Scales **EC2 worker nodes**
* Triggered when:

  * Pods are pending due to lack of resources
  * Nodes are underutilized

Example:

```
Pods want to scale to 15
But nodes only fit 10
â†’ Kubernetes asks AWS for more EC2s
```

ðŸŸ¡ Slower than pod scaling
ðŸŸ¡ Happens only when needed

---

## 4ï¸âƒ£ Where should load balancing happen?

### âœ… **Also BOTH â€” but again, different roles**

---

## ðŸ”¹ External Load Balancing (AWS side)

### ðŸ‘‰ AWS ALB / NLB

* Entry point from the internet
* Routes traffic **into Kubernetes**
* Integrated via:

  * Ingress + AWS Load Balancer Controller

```
Internet â†’ ALB â†’ Kubernetes Service
```

âœ” Handles SSL termination
âœ” AWS-managed, highly available

---

## ðŸ”¹ Internal Load Balancing (Kubernetes side)

### ðŸ‘‰ Kubernetes **Service**

* Distributes traffic **across Pods**
* Uses kube-proxy / IPVS internally

```
Service â†’ Pod1
        â†’ Pod2
        â†’ Pod3
```

âœ” Automatic
âœ” Pod-aware
âœ” Zero config per deployment

---

## 5ï¸âƒ£ What should NOT be done?

âŒ Do **NOT** autoscale EC2s directly for traffic
âŒ Do **NOT** load balance directly to Pods from outside AWS LB
âŒ Do **NOT** treat EC2s as the main scaling unit

Once you adopt Kubernetes:

* **Pods are your scaling unit**
* **EC2s are just capacity providers**


## 6ï¸âƒ£ Final Rule of Thumb (Very Important)

> **Kubernetes scales applications (Pods)**
> **AWS scales infrastructure (EC2 Nodes)**


## 8ï¸âƒ£ One-line answer to your question

> **Apply scaling and load balancing primarily at the Kubernetes Pod level, and let Kubernetes automatically trigger EC2 scaling only when required.**

